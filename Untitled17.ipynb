{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "import calendar\n",
        "class Hierarchy:\n",
        "    def __init__(self, dataframe: pd.DataFrame, ni: int, nd: int) -> None:\n",
        "        \"\"\"\n",
        "        Inicializa a classe Hierarchy.\n",
        "\n",
        "        Args:\n",
        "            dataframe (pd.DataFrame): O DataFrame original para ser processado.\n",
        "            ni (int): O nível de hierarquia desejado para o atributo numérico (idade).\n",
        "            nd (int): O nível de hierarquia desejado para o atributo de data.\n",
        "        \"\"\"\n",
        "        self.df = dataframe.copy()\n",
        "        self.ni = ni\n",
        "        self.nd = nd\n",
        "        self.root = r'data'\n",
        "        self.json_file = 'levels.json'\n",
        "\n",
        "    def construct_hierarchy_attr(self, definitions_levels: list, column_name: str) -> None:\n",
        "        \"\"\"\n",
        "        Constrói a hierarquia para um atributo (coluna) e a salva em um arquivo JSON.\n",
        "\n",
        "        Args:\n",
        "            definitions_levels (list): Uma lista que define como cada nível será dividido.\n",
        "                                       Ex: [1, 5, 10, 'all'] significa 4 níveis de generalização.\n",
        "            column_name (str): O nome da coluna para a qual a hierarquia será criada.\n",
        "        \"\"\"\n",
        "        self.set_quantity_levels = len(definitions_levels)\n",
        "\n",
        "        try:\n",
        "            os.makedirs(self.root, exist_ok=True)\n",
        "        except Exception as e:\n",
        "            print(f\"Ocorreu um erro ao criar a pasta: {e}\")\n",
        "            return\n",
        "\n",
        "        column_values = sorted(self.df[column_name].dropna().unique())\n",
        "\n",
        "        # Estrutura inicial do JSON\n",
        "        json_level = {f'nivel_{level}': None for level in range(self.set_quantity_levels)}\n",
        "\n",
        "        # Gera os intervalos para cada nível de definição\n",
        "        interval_map_level = []\n",
        "        for steps in definitions_levels:\n",
        "            interval_map_level.append(self.compute_pivot(column_values, steps))\n",
        "\n",
        "        # Preenche a estrutura do JSON com os intervalos criados\n",
        "        for h, intervals in enumerate(interval_map_level):\n",
        "            json_values = {str(index + 1): interval for index, interval in enumerate(intervals)}\n",
        "            json_level[f'nivel_{h}'] = json_values\n",
        "\n",
        "        # Salva o dicionário completo no arquivo JSON\n",
        "        try:\n",
        "            with open(self.json_file, 'w', encoding='utf-8') as f:\n",
        "                json.dump(json_level, f, ensure_ascii=False, indent=4)\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao salvar o arquivo JSON: {e}\")\n",
        "\n",
        "    def compute_pivot(self, array: list, num_intervals: int | str):\n",
        "        \"\"\"\n",
        "        Calcula os intervalos (pivots) para um array de valores. (LÓGICA CORRIGIDA)\n",
        "\n",
        "        Args:\n",
        "            array (list): Lista de valores únicos e ordenados.\n",
        "            num_intervals (int | str): O número de intervalos a serem criados ou 'all' para um único intervalo.\n",
        "\n",
        "        Returns:\n",
        "            list: Uma lista de tuplas, onde cada tupla é um intervalo (min, max).\n",
        "        \"\"\"\n",
        "        if not array:\n",
        "            return []\n",
        "\n",
        "        if num_intervals == 'all':\n",
        "            return [(min(array), max(array))]\n",
        "\n",
        "        if num_intervals == 1:\n",
        "            return [(val, val) for val in array]\n",
        "\n",
        "        pivots = []\n",
        "        length = len(array)\n",
        "\n",
        "        num_intervals = min(num_intervals, length)\n",
        "\n",
        "        step = length // num_intervals\n",
        "        remainder = length % num_intervals\n",
        "\n",
        "        start_index = 0\n",
        "        for i in range(num_intervals):\n",
        "            end_index = start_index + step + (1 if i < remainder else 0)\n",
        "\n",
        "            # Garante que o índice final não ultrapasse o limite do array\n",
        "            # O -1 é porque o índice é baseado em zero\n",
        "            pivots.append((array[start_index], array[end_index - 1]))\n",
        "\n",
        "            start_index = end_index\n",
        "\n",
        "        return pivots\n",
        "\n",
        "    def apply_age_hierarchy(self) -> pd.Series:\n",
        "        \"\"\"\n",
        "        Aplica a hierarquia de idade definida no arquivo JSON.\n",
        "\n",
        "        Returns:\n",
        "            pd.Series: Uma série do Pandas com os valores de idade generalizados.\n",
        "        \"\"\"\n",
        "        column_name = 'idadeCaso'\n",
        "        level = self.ni\n",
        "\n",
        "        if level == 0:\n",
        "           # Nível 0 significa sem generalização, retorna os valores originais\n",
        "           return self.df[column_name]\n",
        "\n",
        "        if not (0 < level < self.set_quantity_levels):\n",
        "            print(f'Nível {level} inválido. Escolha um nível entre 1 e {self.set_quantity_levels - 1}.')\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            with open(self.json_file, 'r', encoding='utf-8') as fr:\n",
        "                levels_data = json.load(fr)\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Erro: Arquivo '{self.json_file}' não encontrado. Execute 'construct_hierarchy_attr' primeiro.\")\n",
        "            return None\n",
        "\n",
        "        intervals = list(levels_data.get(f'nivel_{level}', {}).values())\n",
        "        if not intervals:\n",
        "            print(f\"Nenhum intervalo encontrado para o nível {level} no JSON.\")\n",
        "            return None\n",
        "\n",
        "        generalized_column = []\n",
        "\n",
        "        for value in self.df[column_name]:\n",
        "            if pd.isna(value):\n",
        "                generalized_column.append(None)\n",
        "                continue\n",
        "\n",
        "            found = False\n",
        "            for interval in intervals:\n",
        "                if interval[0] <= float(value) <= interval[1]:\n",
        "                    generalized_column.append(f\"{interval[0]}-{interval[1]}\")\n",
        "                    found = True\n",
        "                    break\n",
        "\n",
        "            if not found:\n",
        "                generalized_column.append('Fora do intervalo')\n",
        "\n",
        "        return pd.Series(generalized_column, name=f\"{column_name}_gen_n{level}\")\n",
        "\n",
        "    def apply_date_hierarchy(self) -> pd.Series:\n",
        "        \"\"\"\n",
        "        Aplica a generalização na coluna de data de nascimento.\n",
        "        \"\"\"\n",
        "        level = self.nd\n",
        "        column_name = 'dataNascimento'\n",
        "\n",
        "        date_series = pd.to_datetime(self.df[column_name], errors='coerce')\n",
        "\n",
        "        generalized_dates = None\n",
        "        if level == 0:\n",
        "            generalized_dates = date_series.dt.strftime('%d/%m/%Y')\n",
        "        elif level == 1:\n",
        "            generalized_dates = date_series.dt.strftime('%m/%Y')\n",
        "        elif level == 2:\n",
        "            generalized_dates = date_series.dt.strftime('%Y')\n",
        "        else:\n",
        "            print('Nível de data inválido. Escolha entre 0, 1 ou 2.')\n",
        "            return None\n",
        "\n",
        "        print(f\"\\n✅ Generalização de data (Nível {level}) aplicada com sucesso!\")\n",
        "        return generalized_dates.rename(f\"{column_name}_gen_n{level}\")\n",
        "    def calculate_precision(self, generalized_df: pd.DataFrame) -> float:\n",
        "        \"\"\"\n",
        "        Calcula a métrica de Precisão (Perda de Informação) para o DataFrame generalizado.\n",
        "\n",
        "        Args:\n",
        "            generalized_df (pd.DataFrame): O DataFrame contendo as colunas generalizadas.\n",
        "\n",
        "        Returns:\n",
        "            float: O valor da precisão, onde 1.0 é nenhuma perda de informação.\n",
        "        \"\"\"\n",
        "        attributes = {'idadeCaso': 'idadeCaso_gen_n', 'dataNascimento': 'dataNascimento_gen_n'}\n",
        "        total_information_loss = 0.0\n",
        "\n",
        "        num_records = len(self.df)\n",
        "        num_attributes = len(attributes)\n",
        "\n",
        "        for original_attr, generalized_attr_prefix in attributes.items():\n",
        "            # Encontra o nome completo da coluna generalizada\n",
        "            try:\n",
        "                gen_col_name = [c for c in generalized_df.columns if c.startswith(generalized_attr_prefix)][0]\n",
        "            except IndexError:\n",
        "                print(f\"Aviso: Coluna generalizada para '{original_attr}' não encontrada.\")\n",
        "                continue\n",
        "\n",
        "            # |HGV_Ai|: Tamanho do domínio original (número de valores únicos)\n",
        "            hgv_size = self.df[original_attr].dropna().nunique()\n",
        "            if hgv_size == 0:\n",
        "                continue\n",
        "\n",
        "            for value in generalized_df[gen_col_name]:\n",
        "                h = 1.0  # O padrão é 1 (nenhuma generalização ou valor nulo)\n",
        "\n",
        "                if pd.notna(value):\n",
        "                    if original_attr == 'idadeCaso':\n",
        "                        # Se for um intervalo como \"20-29\"\n",
        "                        if isinstance(value, str) and '-' in value:\n",
        "                            try:\n",
        "                                parts = value.split('-')\n",
        "                                h = float(parts[1]) - float(parts[0]) + 1\n",
        "                            except (ValueError, IndexError):\n",
        "                                h = 1.0 # Caso a string não seja um intervalo válido\n",
        "\n",
        "                    elif original_attr == 'dataNascimento':\n",
        "                        value_str = str(value)\n",
        "                        # Nível 2 (ano): h = número de dias no ano\n",
        "                        if len(value_str) == 4 and value_str.isdigit():\n",
        "                            year = int(value_str)\n",
        "                            h = 366 if calendar.isleap(year) else 365\n",
        "                        # Nível 1 (mês/ano): h = número de dias no mês\n",
        "                        elif len(value_str) == 7 and '/' in value_str:\n",
        "                            try:\n",
        "                                month, year = map(int, value_str.split('/'))\n",
        "                                h = calendar.monthrange(year, month)[1]\n",
        "                            except ValueError:\n",
        "                                h = 1.0\n",
        "\n",
        "                # Soma a perda normalizada para esta célula\n",
        "                total_information_loss += (h / hgv_size)\n",
        "\n",
        "        # Calcula a perda média e a precisão final\n",
        "        average_loss = total_information_loss / (num_records * num_attributes)\n",
        "        precision = 1 - average_loss\n",
        "\n",
        "        return precision\n"
      ],
      "metadata": {
        "id": "LXL3cfVbEYd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import calendar\n",
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "class Anonymizer(Hierarchy):\n",
        "\n",
        "    def __init__(self, dataframe: pd.DataFrame):\n",
        "\n",
        "        #Passando para o init pai o dataframe e ni,nd igual a 0 para fazer generalização grupo a grupo e não globalmente\n",
        "        super().__init__(dataframe, ni=0, nd=0)\n",
        "\n",
        "        # 'self.df_anon' é uma variável de instância que guardará o DataFrame\n",
        "        self.df_anon = None\n",
        "\n",
        "    # Método para preparar os dados\n",
        "    def preprocess_data(self, sample_frac=0.01):\n",
        "        #Faz a seleção de colunas\n",
        "        columns_of_interest = ['idadeCaso', 'dataNascimento', 'racaCor']\n",
        "        #Dropa nulos das 3 colunas e faz uma copia\n",
        "        temp_df = self.df[columns_of_interest].copy()\n",
        "\n",
        "        # Converte a coluna 'idadeCaso' para um formato numérico e se não conseguir converter coloca 'NaN'\n",
        "        temp_df['idadeCaso'] = pd.to_numeric(temp_df['idadeCaso'], errors='coerce')\n",
        "\n",
        "        # Converte a coluna 'dataNascimento' para o formato de data e hora e se não conseguir converter coloca 'NaN'\n",
        "        temp_df['dataNascimento'] = pd.to_datetime(temp_df['dataNascimento'], errors='coerce')\n",
        "\n",
        "        # Ela remove as linhas que se tornaram 'NaN'\n",
        "        temp_df = temp_df.dropna()\n",
        "\n",
        "        #Verifica se a pessoa digitou que queria todo o dataset ou apenas uma fração dele\n",
        "        if sample_frac < 1.0:\n",
        "            print(f\"Reduzindo dataset de {len(temp_df)} para {sample_frac*100}%...\")\n",
        "            #Pega uma fração aleatoria das linhas\n",
        "            self.df_anon = temp_df.sample(frac=sample_frac, random_state=42)\n",
        "\n",
        "        #Caso tiver digitado 1 é por que quer o dataset completo logo vem para o else\n",
        "        else:\n",
        "            self.df_anon = temp_df\n",
        "\n",
        "        #Após usar usar o dropna e o sample os indices ficam bagunçados, dando reset_index reordena os indices\n",
        "        self.df_anon.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        print(f\"Pré-processamento concluído. Registros finais para uso: {len(self.df_anon)}\")\n",
        "\n",
        "        #Retorna o dataset tratado\n",
        "        return self.df_anon\n",
        "\n",
        "    #Normalizar os dados para o knn.\n",
        "    def _get_normalized_data(self):\n",
        "\n",
        "        # 1. Prepara os dados para o scaler.\n",
        "        # Cria uma copia para auxiliar e não mexer no data set\n",
        "        data_to_scale = self.df_anon[['idadeCaso', 'dataNascimento']].copy()\n",
        "\n",
        "        # Converter Data para numero\n",
        "        data_to_scale['dataNascimento'] = data_to_scale['dataNascimento'].astype('int64')\n",
        "\n",
        "        #Normalizando\n",
        "        scaler = MinMaxScaler()\n",
        "        scaled_matrix = scaler.fit_transform(data_to_scale)\n",
        "\n",
        "        # Converte a matriz NumPy de volta para um DataFrame do Pandas.\n",
        "        df_norm = pd.DataFrame(scaled_matrix, columns=['age_norm', 'date_norm'])\n",
        "\n",
        "        #Retorna o DataFrame normalizado.\n",
        "        return df_norm\n",
        "\n",
        "    #Algoritmo k-NN para criar os grupos.\n",
        "    def create_clusters_knn(self, k: int):\n",
        "        \"\"\"\n",
        "        Agrupa todos os registros em \"clusters\" (grupos) de tamanho 'k' ou mais.\n",
        "        Ele faz isso pegando um registro, encontrando seus 'k-1' vizinhos\n",
        "        mais próximos e colocando todos no mesmo grupo.\n",
        "        \"\"\"\n",
        "        print(f\"Agrupamento k-NN para k={k}...\")\n",
        "\n",
        "        # 'df_calc': Pega o DataFrame normalizado do método anterior.\n",
        "        df_calc = self._get_normalized_data()\n",
        "\n",
        "        # Cria index hash para facilitar a exclusão após agrupar\n",
        "        available_indices = set(df_calc.index)\n",
        "\n",
        "        # A lista que irá armazenar todos os grupos\n",
        "        clusters = []\n",
        "\n",
        "        # Troca o dataframe por uma matriz numpy para agilizar os calculos\n",
        "        data_matrix = df_calc.values\n",
        "\n",
        "        # Uma lista que mapeia a posição na matriz NumPy\n",
        "        index_map = list(df_calc.index)\n",
        "\n",
        "        # Realiza o agrupamento até o ultimo grupo de tamanho k\n",
        "        while len(available_indices) >= k:\n",
        "\n",
        "            #Pega o primeiro indice que achar dentro do available_indices\n",
        "            pivot_idx_original = next(iter(available_indices))\n",
        "\n",
        "            #Encontra a posição dentro da matriz\n",
        "            pivot_pos = index_map.index(pivot_idx_original)\n",
        "\n",
        "            # Pega as coordenadas de idade e data normalizadas do pivô.\n",
        "            pivot_coords = data_matrix[pivot_pos]\n",
        "\n",
        "            #Calula a distância Euclidiana para toda a matriz\n",
        "            dists = np.sqrt(((data_matrix - pivot_coords) ** 2).sum(axis=1))\n",
        "\n",
        "            # Uma lista temporária para guardar os vizinhos.\n",
        "            candidates = []\n",
        "\n",
        "            # 'for pos, dist in enumerate(dists)': Este loop itera sobre\n",
        "            # todas as distâncias que acabamos de calcular.\n",
        "            for pos, dist in enumerate(dists):\n",
        "\n",
        "                #Converte a posição da matriz de volta para o indice do data frame\n",
        "                idx_orig = index_map[pos]\n",
        "\n",
        "                #Faz a verificação se o vizinho já está na lista dos indices disponiveis,\n",
        "                #Caso estiver adiciona a lista para agrupar\n",
        "                if idx_orig in available_indices:\n",
        "                    candidates.append((dist, idx_orig))\n",
        "\n",
        "            #Ordena a lista de vizinhos atraves da distancia, os mais proximos ficam no topo\n",
        "            candidates.sort(key=lambda x: x[0])\n",
        "\n",
        "            # Seleciona os k vizinhos mais próximos\n",
        "            best_k = candidates[:k]\n",
        "\n",
        "            #Extrai apenas os índices dos k melhores vizinhos.\n",
        "            group_indices = [c[1] for c in best_k]\n",
        "\n",
        "            #Adiciona o grupo a lista de agrupamentos\n",
        "            clusters.append(group_indices)\n",
        "\n",
        "\n",
        "            #Exluí os indices que já foram usados\n",
        "            for idx in group_indices:\n",
        "                available_indices.remove(idx)\n",
        "\n",
        "        #O If é execcutado após o while terminar, ele cuida das linhas que sobraram.\n",
        "        if available_indices:\n",
        "            #Se o dataset inteiro tiver menos que k linhas cria apenas um agrupamento\n",
        "            if not clusters:\n",
        "                clusters.append(list(available_indices))\n",
        "\n",
        "            else:\n",
        "                #Pega o ultimo agrupamento e adiciona as sobras a ele\n",
        "                clusters[-1].extend(list(available_indices))\n",
        "\n",
        "        print(f\"Agrupamento concluído. Total de clusters: {len(clusters)}\")\n",
        "\n",
        "        # Retorna a lista de grupos\n",
        "        return clusters\n",
        "\n",
        "\n",
        "    # Método para aplicar a generalização nos grupos.\n",
        "    def apply_k_anonymity(self, clusters):\n",
        "        \"\"\"\n",
        "        Recebe os 'clusters' (listas de índices) e o DataFrame original.\n",
        "        Para cada cluster, ele olha os valores de idade e data e os\n",
        "        generaliza para que sejam IDÊNTICOS para todo o grupo.\n",
        "\n",
        "        Retorna: 'df_k_anonymized' (um novo DataFrame com os valores\n",
        "                 de idade/data generalizados).\n",
        "        \"\"\"\n",
        "        print(\"Aplicando generalização (k-anonimato)...\")\n",
        "\n",
        "        #Uma lista vazia que vai guardar as novas linhas antes de virarem um dataframe\n",
        "        anonymized_rows = []\n",
        "\n",
        "        # 'for cluster_indices in clusters': O loop principal.\n",
        "        # Ele itera sobre cada grupo de índices (ex: [0, 5, 8])\n",
        "        # que o 'create_clusters_knn' gerou.\n",
        "        for cluster_indices in clusters:\n",
        "\n",
        "            # 'subset = self.df_anon.loc[cluster_indices]': Pega o\n",
        "            # sub-DataFrame (as linhas de dados reais) para os índices\n",
        "            # deste grupo específico.\n",
        "            subset = self.df_anon.loc[cluster_indices]\n",
        "\n",
        "            # --- 1. Generalização de IDADE ---\n",
        "\n",
        "            # 'ages = subset['idadeCaso']': Pega todas as idades deste grupo.\n",
        "            ages = subset['idadeCaso']\n",
        "\n",
        "            # 'if ages.min() == ages.max()': Este 'if' checa se\n",
        "            # todas as idades no grupo são idênticas.\n",
        "            if ages.min() == ages.max():\n",
        "                # 'age_gen': A idade generalizada. Se forem idênticas,\n",
        "                # apenas converte o número para string (ex: \"65\").\n",
        "                age_gen = str(int(ages.min()))\n",
        "\n",
        "            # 'else': Se as idades forem diferentes (ex: 65, 66, 68).\n",
        "            else:\n",
        "                # 'age_gen': Cria um intervalo usando o mínimo e o máximo\n",
        "                # (ex: \"[65-68]\").\n",
        "                age_gen = f\"[{int(ages.min())}-{int(ages.max())}]\"\n",
        "\n",
        "            # --- 2. Generalização de DATA ---\n",
        "\n",
        "            # 'dates = subset['dataNascimento']': Pega todas as datas deste grupo.\n",
        "            dates = subset['dataNascimento']\n",
        "\n",
        "            # 'if dates.nunique() == 1': Este 'if' checa se todas as datas\n",
        "            # são *exatamente* iguais (mesmo dia, mês e ano).\n",
        "            if dates.nunique() == 1:\n",
        "                # 'date_gen': Formata a data como \"AAAA-MM-DD\".\n",
        "                date_gen = dates.iloc[0].strftime('%Y-%m-%d')\n",
        "\n",
        "            # 'elif dates.dt.to_period('M').nunique() == 1': Se não, checa\n",
        "            # se todas as datas caem no *mesmo mês e ano*.\n",
        "            elif dates.dt.to_period('M').nunique() == 1:\n",
        "                # 'date_gen': Generaliza, mantendo apenas o mês/ano (ex: \"02/1955\").\n",
        "                date_gen = dates.iloc[0].strftime('%m/%Y')\n",
        "\n",
        "            # 'elif dates.dt.to_period('Y').nunique() == 1': Se não, checa\n",
        "            # se todas as datas caem no *mesmo ano*.\n",
        "            elif dates.dt.to_period('Y').nunique() == 1:\n",
        "                # 'date_gen': Generaliza, mantendo apenas o ano (ex: \"1955\").\n",
        "                date_gen = dates.iloc[0].strftime('%Y')\n",
        "\n",
        "            # 'else': Se até os anos forem diferentes.\n",
        "            else:\n",
        "                # 'date_gen': Cria um intervalo de anos (ex: \"1955-1957\").\n",
        "                min_y = dates.dt.year.min()\n",
        "                max_y = dates.dt.year.max()\n",
        "                date_gen = f\"{min_y}-{max_y}\"\n",
        "\n",
        "            # --- 3. Construção das linhas anonimizadas ---\n",
        "\n",
        "            # 'for idx in cluster_indices': Este loop itera sobre\n",
        "            # cada índice *original* do grupo.\n",
        "            for idx in cluster_indices:\n",
        "\n",
        "                # 'original_row = self.df_anon.loc[idx]': Pega a linha\n",
        "                # original para recuperar o atributo sensível.\n",
        "                original_row = self.df_anon.loc[idx]\n",
        "\n",
        "                # 'anonymized_rows.append(...)': Adiciona um novo dicionário\n",
        "                # (que será uma linha no novo DataFrame).\n",
        "                # Note: 'idadeCaso' e 'dataNascimento' são os valores\n",
        "                # GENERALIZADOS, mas 'racaCor' é o valor ORIGINAL.\n",
        "                anonymized_rows.append({\n",
        "                    'idadeCaso': age_gen,           # Valor generalizado\n",
        "                    'dataNascimento': date_gen,     # Valor generalizado\n",
        "                    'racaCor': original_row['racaCor'] # Valor original\n",
        "                })\n",
        "\n",
        "        # 'df_k_anonymized = pd.DataFrame(anonymized_rows)': Converte a\n",
        "        # lista de dicionários em um DataFrame completo.\n",
        "        df_k_anonymized = pd.DataFrame(anonymized_rows)\n",
        "\n",
        "        print(f\"Generalização concluída. Dataset anonimizado gerado com {len(df_k_anonymized)} linhas.\")\n",
        "\n",
        "        # 'return df_k_anonymized': Retorna o DataFrame k-anonimizado.\n",
        "        return df_k_anonymized\n",
        "\n",
        "\n",
        "    # Método para aplicar a l-Diversidade.\n",
        "    def apply_l_diversity(self, df_k_anon: pd.DataFrame, l_val: int):\n",
        "        \"\"\"\n",
        "        Recebe o DataFrame k-anonimizado e o valor 'l'.\n",
        "        Ele garante que, para cada classe de equivalência (grupo),\n",
        "        existam pelo menos 'l' valores distintos para 'racaCor'.\n",
        "\n",
        "        Retorna: 'df_l_div' (o DataFrame final, agora k-anônimo E l-diverso).\n",
        "        \"\"\"\n",
        "        print(f\"\\nAplicando l-diversidade para l={l_val}...\")\n",
        "\n",
        "        # 'VALID_RACES': A lista de raças permitidas, conforme\n",
        "        # a especificação do trabalho.\n",
        "        VALID_RACES = [\"Branca\", \"Preta\", \"Parda\", \"Indígena\", \"Asiática\"]\n",
        "\n",
        "        # 'df_l_div': Cria uma cópia do DataFrame k-anonimizado para\n",
        "        # podermos modificá-lo sem alterar o original.\n",
        "        df_l_div = df_k_anon.copy()\n",
        "\n",
        "        # 'df_l_div['racaCor'] = ...': Este é um passo de limpeza global.\n",
        "        # Ele substitui 'Amarela' por 'Asiática' em TODO o DataFrame\n",
        "        # ANTES de começar a processar os grupos.\n",
        "        df_l_div['racaCor'] = df_l_div['racaCor'].replace('Amarela', 'Asiática')\n",
        "\n",
        "        # 'groups = df_l_div.groupby(...)': Esta é a parte chave.\n",
        "        # Ele agrupa o DataFrame pelas colunas generalizadas.\n",
        "        # 'groups' agora é um objeto que contém todas as \"classes de equivalência\".\n",
        "        groups = df_l_div.groupby(['idadeCaso', 'dataNascimento'])\n",
        "\n",
        "        # 'for name, group_df in groups': Este loop itera sobre cada\n",
        "        # classe de equivalência. 'name' é a tupla (ex: (\"[65-68]\", \"02/1955\"))\n",
        "        # e 'group_df' é o sub-DataFrame com todas as linhas desse grupo.\n",
        "        for name, group_df in groups:\n",
        "\n",
        "            # 'group_indices': Pega os índices das linhas deste grupo.\n",
        "            group_indices = group_df.index\n",
        "\n",
        "            # 1. 'current_races': Pega a série de 'racaCor' para este grupo.\n",
        "            current_races = df_l_div.loc[group_indices, 'racaCor']\n",
        "\n",
        "            # 2. 'valid_races_in_group': Filtra, mantendo apenas as raças\n",
        "            # que estão na nossa lista 'VALID_RACES'. \"Ignorado\" é descartado.\n",
        "            valid_races_in_group = current_races[current_races.isin(VALID_RACES)]\n",
        "\n",
        "            # 'distinct_set': Encontra as raças *únicas* válidas\n",
        "            # (ex: {\"Parda\", \"Branca\"}).\n",
        "            distinct_set = set(valid_races_in_group)\n",
        "\n",
        "            # 'num_distinct': Conta quantas raças únicas válidas existem (ex: 2).\n",
        "            num_distinct = len(distinct_set)\n",
        "\n",
        "            # 3. 'if num_distinct >= l_val': Este é o \"caminho feliz\".\n",
        "            # O grupo JÁ satisfaz a l-diversidade (ex: l=2 e num_distinct=2).\n",
        "            if num_distinct >= l_val:\n",
        "\n",
        "                # 'invalid_mask = ...': Mesmo que o grupo seja diverso,\n",
        "                # ele pode conter valores \"sujos\" (ex: \"Ignorado\").\n",
        "                # Esta máscara encontra esses valores.\n",
        "                invalid_mask = ~current_races.isin(VALID_RACES)\n",
        "\n",
        "                # 'for idx in group_indices[invalid_mask]': Um loop\n",
        "                # para limpar esses valores \"sujos\".\n",
        "                for idx in group_indices[invalid_mask]:\n",
        "\n",
        "                    # 'df_l_div.loc[idx, 'racaCor'] = ...': Substitui\n",
        "                    # o valor \"sujo\" (ex: \"Ignorado\") por uma raça aleatória\n",
        "                    # que JÁ EXISTE no grupo (ex: \"Parda\" ou \"Branca\").\n",
        "                    df_l_div.loc[idx, 'racaCor'] = random.choice(list(distinct_set))\n",
        "\n",
        "                # 'continue': Pula para a próxima classe de equivalência.\n",
        "                # O trabalho com este grupo terminou.\n",
        "                continue\n",
        "\n",
        "            # 4. 'needed = l_val - num_distinct': Este é o \"caminho difícil\".\n",
        "            # O grupo NÃO é diverso. Calculamos quantas raças *novas*\n",
        "            # precisamos adicionar (ex: l=3, num_distinct=1 -> needed=2).\n",
        "            needed = l_val - num_distinct\n",
        "\n",
        "            # 'available_new_races': Lista de raças que podemos inserir.\n",
        "            # É a lista de 'VALID_RACES' MENOS as raças que já estão no grupo.\n",
        "            available_new_races = list(set(VALID_RACES) - distinct_set)\n",
        "\n",
        "            # 'random.shuffle(...)': Embaralha a lista para que a inserção\n",
        "            # seja aleatória (ex: não inserir \"Branca\" sempre).\n",
        "            random.shuffle(available_new_races)\n",
        "\n",
        "            # 'races_to_add': Pega o número 'needed' de raças da lista\n",
        "            # embaralhada (ex: [\"Branca\", \"Preta\"]).\n",
        "            races_to_add = available_new_races[:needed]\n",
        "\n",
        "            # 5. 'is_invalid', 'is_duplicate': Encontra as linhas que\n",
        "            # são \"candidatas\" a serem sobrescritas.\n",
        "            # Prioridade 1: 'is_invalid' (linhas com \"Ignorado\", etc.)\n",
        "            # Prioridade 2: 'is_duplicate' (ex: a 2ª, 3ª, 4ª \"Parda\" do grupo)\n",
        "            is_invalid = ~current_races.isin(VALID_RACES)\n",
        "            is_duplicate = current_races.duplicated()\n",
        "\n",
        "            # 'replaceable_mask': Combina as duas máscaras.\n",
        "            replaceable_mask = is_invalid | is_duplicate\n",
        "\n",
        "            # 'replaceable_indices': A lista de índices que podemos sobrescrever.\n",
        "            replaceable_indices = list(group_indices[replaceable_mask])\n",
        "\n",
        "            # 'if len(replaceable_indices) < needed': Este 'if' trata\n",
        "            # um caso difícil: E se 'needed=2', mas só temos 1 linha\n",
        "            # \"substituível\"? (ex: grupo [Parda, Branca] com k=4, l=3).\n",
        "            if len(replaceable_indices) < needed:\n",
        "                # 'other_indices': Pega os índices que *não* eram\n",
        "                # substituíveis (ex: a *primeira* \"Parda\" e a *primeira* \"Branca\").\n",
        "                other_indices = list(group_indices[~replaceable_mask])\n",
        "                random.shuffle(other_indices)\n",
        "                # Adiciona esses índices \"difíceis\" à lista de candidatos.\n",
        "                replaceable_indices.extend(other_indices)\n",
        "\n",
        "            # 'if not replaceable_indices': Verificação de segurança.\n",
        "            # Se não houver linhas para substituir (caso muito raro), avisa e pula.\n",
        "            if not replaceable_indices:\n",
        "                print(f\"Aviso: Grupo {name} não tem linhas substituíveis. Impossível garantir l={l_val}.\")\n",
        "                continue\n",
        "\n",
        "            # 'if not races_to_add': Verificação de segurança.\n",
        "            # Se não houver raças novas para adicionar (ex: k=4, l=5), avisa e pula.\n",
        "            if not races_to_add:\n",
        "                print(f\"Aviso: Grupo {name} (r={distinct_set}) não tem raças novas disponíveis. Máximo de {len(VALID_RACES)} atingido.\")\n",
        "                continue\n",
        "\n",
        "            # 6. 'for i in range(needed)': O loop de substituição.\n",
        "            # Ele roda 'needed' vezes (ex: 2 vezes).\n",
        "            for i in range(needed):\n",
        "\n",
        "                # 'if i >= len(...)': Verificação de segurança para\n",
        "                # não dar erro de índice (ex: se l > k).\n",
        "                if i >= len(races_to_add) or i >= len(replaceable_indices):\n",
        "                    break\n",
        "\n",
        "                # 'row_idx_to_change': Pega o índice da linha a ser mudada.\n",
        "                row_idx_to_change = replaceable_indices[i]\n",
        "                # 'new_race': Pega a raça nova a ser inserida.\n",
        "                new_race = races_to_add[i]\n",
        "\n",
        "                # 'df_l_div.loc[...] = new_race': A substituição!\n",
        "                # Troca o valor antigo (ex: \"Parda\") pelo novo (ex: \"Branca\").\n",
        "                df_l_div.loc[row_idx_to_change, 'racaCor'] = new_race\n",
        "\n",
        "            # 7. 'final_races = ...': Esta é a limpeza final.\n",
        "            # Após as substituições, pode ter sobrado algum \"Ignorado\".\n",
        "            final_races = df_l_div.loc[group_indices, 'racaCor']\n",
        "            final_invalid_mask = ~final_races.isin(VALID_RACES)\n",
        "\n",
        "            # 'final_valid_set': Pega o *novo* conjunto de raças válidas\n",
        "            # do grupo (que agora é diverso).\n",
        "            final_valid_set = list(set(final_races[~final_invalid_mask]))\n",
        "\n",
        "            # 'if not final_valid_set': Verificação de segurança. Se o\n",
        "            # grupo só tinha \"Ignorado\", usa a lista mestre.\n",
        "            if not final_valid_set: final_valid_set = VALID_RACES # Failsafe\n",
        "\n",
        "            # 'for idx in group_indices[final_invalid_mask]': Loop final\n",
        "            # para limpar qualquer \"Ignorado\" restante.\n",
        "            for idx in group_indices[final_invalid_mask]:\n",
        "                # Substitui por uma raça aleatória do *novo* conjunto diverso.\n",
        "                df_l_div.loc[idx, 'racaCor'] = random.choice(final_valid_set)\n",
        "\n",
        "        print(\"Aplicação de l-diversidade concluída.\")\n",
        "\n",
        "        # 'return df_l_div': Retorna o DataFrame final, k-anônimo e l-diverso.\n",
        "        return df_l_div\n",
        "\n",
        "    # Método para calcular a métrica de precisão (perda de informação).\n",
        "    def calculate_precision(self, generalized_df: pd.DataFrame) -> float:\n",
        "        \"\"\"\n",
        "        Calcula a Perda de Informação (ILoss) e retorna a Precisão (1 - ILoss).\n",
        "        Compara os valores generalizados (ex: \"[20-29]\") com o domínio\n",
        "        original (ex: 85 idades únicas) para medir quanta informação foi perdida.\n",
        "\n",
        "        Retorna: 'precision' (um float, ex: 0.9875). 1.0 é perfeito (sem perda).\n",
        "        \"\"\"\n",
        "\n",
        "        # 'attributes': As colunas que são generalizadas (QIs).\n",
        "        # 'racaCor' não entra, pois é atributo sensível.\n",
        "        attributes = ['idadeCaso', 'dataNascimento']\n",
        "\n",
        "        # 'total_information_loss': Uma variável para acumular a\n",
        "        # perda de informação normalizada de CADA CÉLULA no DataFrame.\n",
        "        total_information_loss = 0.0\n",
        "\n",
        "        # 'num_records': O número total de linhas que estamos processando.\n",
        "        num_records = len(self.df_anon)\n",
        "\n",
        "        # 'if num_records == 0': Verificação de segurança para evitar divisão por zero.\n",
        "        if num_records == 0:\n",
        "            return 0.0\n",
        "\n",
        "        # 'num_attributes': O número de colunas que estamos medindo (neste caso, 2).\n",
        "        num_attributes = len(attributes)\n",
        "\n",
        "        # 'for attr_name in attributes': Loop externo.\n",
        "        # Primeiro calcula a perda para 'idadeCaso', depois para 'dataNascimento'.\n",
        "        for attr_name in attributes:\n",
        "\n",
        "            # 'hgv_size': O tamanho do \"domínio\" original.\n",
        "            # Conta quantos valores ÚNICOS existiam para 'idadeCaso'\n",
        "            # no DataFrame *antes* da generalização (ex: 85 idades únicas).\n",
        "            hgv_size = self.df_anon[attr_name].dropna().nunique()\n",
        "\n",
        "            # 'if hgv_size <= 1': Verificação de segurança. Se só havia 1 valor\n",
        "            # único, a perda é impossível (é 0). Pula para o próximo atributo.\n",
        "            if hgv_size <= 1:\n",
        "                continue\n",
        "\n",
        "            # 'for value in generalized_df[attr_name]': Loop interno.\n",
        "            # Itera sobre CADA CÉLULA na coluna *generalizada*\n",
        "            # (ex: \"[20-29]\", \"[20-29]\", \"65\", \"[30-35]\", ...).\n",
        "            for value in generalized_df[attr_name]:\n",
        "\n",
        "                # 'h = 1.0': A \"perda\" padrão para uma célula.\n",
        "                # Se o valor for \"65\" (não generalizado), h=1.\n",
        "                h = 1.0\n",
        "\n",
        "                # 'if pd.notna(value)': Só calcula a perda se o valor\n",
        "                # não for Nulo (NaN).\n",
        "                if pd.notna(value):\n",
        "\n",
        "                    # 'if attr_name == 'idadeCaso'': Lógica de perda para Idade.\n",
        "                    if attr_name == 'idadeCaso':\n",
        "                        value_str = str(value)\n",
        "\n",
        "                        # 'if value_str.startswith('[') ...': Checa se é\n",
        "                        # um intervalo (ex: \"[20-29]\").\n",
        "                        if value_str.startswith('[') and '-' in value_str:\n",
        "                            try:\n",
        "                                # Extrai os números do intervalo.\n",
        "                                parts = value_str.strip('[]').split('-')\n",
        "                                # 'h = ...': A perda é o tamanho do intervalo.\n",
        "                                # Ex: 29 - 20 + 1 = 10.\n",
        "                                h = float(parts[1]) - float(parts[0]) + 1\n",
        "                            except (ValueError, IndexError):\n",
        "                                h = 1.0 # F-allback\n",
        "\n",
        "                    # 'elif attr_name == 'dataNascimento'': Lógica de perda para Data.\n",
        "                    elif attr_name == 'dataNascimento':\n",
        "                        value_str = str(value)\n",
        "\n",
        "                        # 'if len(value_str) == 4 ...': Se for um ano (ex: \"1973\").\n",
        "                        if len(value_str) == 4 and value_str.isdigit(): # \"1973\"\n",
        "                            year = int(value_str)\n",
        "                            # 'h = ...': A perda é o número de dias naquele ano.\n",
        "                            h = 366 if calendar.isleap(year) else 365\n",
        "\n",
        "                        # 'elif len(value_str) == 9 ...': Se for um intervalo de anos (ex: \"1970-1973\").\n",
        "                        elif len(value_str) == 9 and '-' in value_str: # \"1970-1973\"\n",
        "                            try:\n",
        "                                y_min, y_max = map(int, value_str.split('-'))\n",
        "                                h = 0 # Acumula os dias\n",
        "                                # 'for y in range(...)': Soma os dias de\n",
        "                                # *todos* os anos no intervalo.\n",
        "                                for y in range(y_min, y_max + 1):\n",
        "                                    h += 366 if calendar.isleap(y) else 365\n",
        "                            except ValueError:\n",
        "                                h = hgv_size # Pior caso\n",
        "\n",
        "                        # 'elif len(value_str) == 7 ...': Se for mês/ano (ex: \"02/1955\").\n",
        "                        elif len(value_str) == 7 and '/' in value_str:\n",
        "                            try:\n",
        "                                month, year = map(int, value_str.split('/'))\n",
        "                                # 'h = ...': A perda é o número de dias\n",
        "                                # naquele mês específico.\n",
        "                                h = calendar.monthrange(year, month)[1]\n",
        "                            except ValueError:\n",
        "                                h = 1.0\n",
        "                        # Se for \"AAAA-MM-DD\", a perda é 1.0 (o 'h' padrão).\n",
        "\n",
        "                # 'total_information_loss += (h / hgv_size)': A fórmula principal.\n",
        "                # A perda *desta célula* é 'h' (seu tamanho) dividido pelo\n",
        "                # 'hgv_size' (tamanho total do domínio).\n",
        "                # Acumula isso ao total.\n",
        "                total_information_loss += (h / hgv_size)\n",
        "\n",
        "        # 'if (num_records * num_attributes) == 0': Verificação de segurança\n",
        "        # para evitar divisão por zero.\n",
        "        if (num_records * num_attributes) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        # 'average_loss = ...': Calcula a perda média por célula.\n",
        "        # É a perda total dividida pelo número total de células (linhas * colunas).\n",
        "        average_loss = total_information_loss / (num_records * num_attributes)\n",
        "\n",
        "        # 'precision = 1 - average_loss': Inverte a perda para obter a precisão.\n",
        "        precision = 1 - average_loss\n",
        "\n",
        "        # 'return precision': Retorna o valor final da métrica.\n",
        "        return precision\n",
        "\n",
        "\n",
        "    # Método final para gerar os relatórios (métricas e gráficos).\n",
        "    def generate_metrics_and_plots(self, df_final: pd.DataFrame, k: int, l: int):\n",
        "        \"\"\"\n",
        "        Calcula as métricas finais (Precisão, Tamanho Médio) e gera os\n",
        "        dois histogramas exigidos pelo trabalho (tamanho das classes k\n",
        "        e diversidade das classes l). Salva os gráficos como .png.\n",
        "        \"\"\"\n",
        "        print(\"\\n--- Métricas Finais ---\")\n",
        "\n",
        "        # 1. 'precision_score = ...': Chama o método que acabamos de ver\n",
        "        # para calcular a precisão do DataFrame final.\n",
        "        precision_score = self.calculate_precision(df_final)\n",
        "        print(f\"Precisão (1 - Perda de Informação): {precision_score:.4f}\")\n",
        "\n",
        "        # 2. 'eq_classes = ...': Re-agrupa o DataFrame final\n",
        "        # para encontrar as classes de equivalência.\n",
        "        eq_classes = df_final.groupby(['idadeCaso', 'dataNascimento'])\n",
        "\n",
        "        # 'num_classes': O número total de grupos distintos.\n",
        "        num_classes = len(eq_classes)\n",
        "        print(f\"Total de Classes de Equivalência: {num_classes}\")\n",
        "\n",
        "        # 3. 'if num_classes > 0': Verificação de segurança.\n",
        "        if num_classes > 0:\n",
        "            # 'avg_size': Tamanho médio de uma classe (Total de linhas / Total de classes).\n",
        "            avg_size = len(df_final) / num_classes\n",
        "            print(f\"Tamanho Médio das Classes: {avg_size:.2f}\")\n",
        "\n",
        "        # 4. Histograma de Tamanhos (k-anonimato)\n",
        "\n",
        "        # 'class_sizes = eq_classes.size()': Obtém uma Série (lista)\n",
        "        # contendo o tamanho de *cada* classe (ex: [4, 4, 6, 8, 4, ...]).\n",
        "        class_sizes = eq_classes.size()\n",
        "\n",
        "        # 'plt.figure(...)': Cria uma nova figura (um gráfico) em branco.\n",
        "        plt.figure(figsize=(10, 6))\n",
        "\n",
        "        # 'bins = range(...)': Define as \"caixas\" do histograma.\n",
        "        # Elas começam em 'k' e vão até o tamanho máximo de classe encontrado.\n",
        "        bins = range(k, class_sizes.max() + 2)\n",
        "\n",
        "        # 'plt.hist(...)': Cria o histograma. Ele conta quantas classes\n",
        "        # caem em cada \"caixa\" (bin).\n",
        "        plt.hist(class_sizes, bins=bins, align='left', edgecolor='black')\n",
        "\n",
        "        # Define os rótulos do gráfico.\n",
        "        plt.title(f'Histograma: Tamanho das Classes de Equivalência (k={k})')\n",
        "        plt.xlabel('Tamanho da Classe (>= k)')\n",
        "        plt.ylabel('Frequência (Nº de Classes)')\n",
        "\n",
        "        # 'plt.xticks(...)': Ajusta os números no eixo X para\n",
        "        # não ficarem sobrepostos e ilegíveis.\n",
        "        plt.xticks(range(k, class_sizes.max() + 1, max(1, (class_sizes.max() - k + 1)//15)))\n",
        "\n",
        "        # 'plot_filename_k': Define o nome do arquivo de saída.\n",
        "        plot_filename_k = rf'data\\hist_k_anon_k{k}.png'\n",
        "\n",
        "        # 'plt.savefig(...)': Salva o gráfico como um arquivo .png.\n",
        "        plt.savefig(plot_filename_k)\n",
        "        print(f\"Gráfico de k-anonimato salvo em: {plot_filename_k}\")\n",
        "\n",
        "        # 'plt.close()': Fecha a figura para liberar memória.\n",
        "        plt.close()\n",
        "\n",
        "        # 5. Histograma de Diversidade (l-diversidade)\n",
        "\n",
        "        # 'class_diversity = ...': Obtém uma Série (lista) contendo\n",
        "        # a contagem de raças *únicas* de cada classe (ex: [2, 3, 2, 4, 2, ...]).\n",
        "        class_diversity = eq_classes['racaCor'].nunique()\n",
        "\n",
        "        # 'plt.figure(...)': Cria o segundo gráfico em branco.\n",
        "        plt.figure(figsize=(10, 6))\n",
        "\n",
        "        # 'diversity_counts = ...': Conta as contagens.\n",
        "        # Ex: \"100 classes têm 2 raças, 50 classes têm 3 raças...\".\n",
        "        diversity_counts = class_diversity.value_counts().sort_index()\n",
        "\n",
        "        # 'plt.bar(...)': Cria um gráfico de *barras* (não histograma)\n",
        "        # para mostrar essas contagens.\n",
        "        plt.bar(diversity_counts.index, diversity_counts.values, edgecolor='black', width=0.8)\n",
        "\n",
        "        # Define os rótulos do gráfico.\n",
        "        plt.title(f'Histograma: Diversidade de Raça (k={k}, l={l})')\n",
        "        plt.xlabel('Nº de Raças Distintas por Classe')\n",
        "        plt.ylabel('Frequência (Nº de Classes)')\n",
        "\n",
        "        # 'plt.xticks(...)': Ajusta os números no eixo X.\n",
        "        plt.xticks(range(min(l, diversity_counts.index.min()), diversity_counts.index.max() + 2))\n",
        "\n",
        "        # 'plot_filename_l': Define o nome do arquivo de saída.\n",
        "        plot_filename_l = rf'data\\hist_l_div_k{k}_l{l}.png'\n",
        "\n",
        "        # 'plt.savefig(...)': Salva o segundo gráfico.\n",
        "        plt.savefig(plot_filename_l)\n",
        "        print(f\"Gráfico de l-diversidade salvo em: {plot_filename_l}\")\n",
        "\n",
        "        # 'plt.close()': Fecha a segunda figura.\n",
        "        plt.close()"
      ],
      "metadata": {
        "id": "HpQfBMjD5_Mb"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_readme():\n",
        "  #Inicializar o content como vazio\n",
        "  content = \"\"\n",
        "  try:\n",
        "      with open(\"Readme.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "          f.write(content)\n",
        "      print(\"\\nReadme.txt criado com sucesso.\")\n",
        "  except Exception as e:\n",
        "      print(f\"Erro ao criar Readme.txt: {e}\")\n",
        "\n",
        "def main():\n",
        "    print(\"##### Início do Processo de Anonimização #####\")\n",
        "\n",
        "    # Parâmetros do Trabalho\n",
        "    #k_values = [2, 4, 8, 16]\n",
        "    #l_values = [2, 3, 4]\n",
        "    k_values = [4]\n",
        "    l_values = [3]\n",
        "    #diminuir o tamanho do dataset\n",
        "    SAMPLE_SIZE = 0.01\n",
        "\n",
        "    try:\n",
        "        df_raw = pd.read_csv('dados_covid-ce_trab02.csv', encoding='latin-1', low_memory=False)\n",
        "    except FileNotFoundError:\n",
        "        print(\"Erro: Arquivo 'dados_covid-ce_trab02.csv' não encontrado.\")\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao ler CSV: {e}\")\n",
        "        return\n",
        "\n",
        "    # Garante que a pasta /data exista (onde os CSVs e PNGs serão salvos)\n",
        "    os.makedirs('data', exist_ok=True)\n",
        "\n",
        "    # 1. Instancia e Prepara os dados UMA VEZ\n",
        "    anon = Anonymizer(df_raw)\n",
        "    print(f\"Processando com {SAMPLE_SIZE*100}% dos dados...\")\n",
        "    anon.preprocess_data(sample_frac=SAMPLE_SIZE)\n",
        "\n",
        "    if anon.df_anon is None or anon.df_anon.empty:\n",
        "        print(\"Pré-processamento falhou ou resultou em dados vazios.\")\n",
        "        return\n",
        "\n",
        "    # Guarda os clusters de cada K para não recalcular\n",
        "    # Nota: A geração do histograma K foi movida para dentro do loop de L\n",
        "    # para garantir que ele seja gerado para cada k/l\n",
        "\n",
        "    # 2. Loop principal\n",
        "    for k in k_values:\n",
        "        print(f\"\\n#################################################\")\n",
        "        print(f\"### Processando para K = {k}\")\n",
        "        print(f\"#################################################\")\n",
        "\n",
        "        # 2.1. k-Anonimato\n",
        "        # Gera os clusters e o dataframe k-anonimizado\n",
        "        clusters = anon.create_clusters_knn(k)\n",
        "        df_k_anon = anon.apply_k_anonymity(clusters)\n",
        "\n",
        "        # 2.2. Loop de l-Diversidade\n",
        "        for l in l_values:\n",
        "            # Regra: l deve ser <= k\n",
        "            if l > k:\n",
        "                print(f\"\\n--- Pulando (k={k}, l={l}) pois l > k.\")\n",
        "                continue\n",
        "\n",
        "            print(f\"\\n--- Aplicando L = {l} (para K = {k}) ---\")\n",
        "\n",
        "            # Aplica l-diversidade (partindo do df_k_anon limpo)\n",
        "            df_final = anon.apply_l_diversity(df_k_anon.copy(), l)\n",
        "\n",
        "            # Salva o arquivo CSV\n",
        "            output_filename = rf'data\\dados covid {k} {l}.csv'\n",
        "            try:\n",
        "                df_final.to_csv(output_filename, index=False, encoding='utf-8')\n",
        "                print(f\"\\nArquivo salvo com sucesso em: {output_filename}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Erro ao salvar CSV '{output_filename}': {e}\")\n",
        "\n",
        "            # Gera Métricas e Plots\n",
        "            anon.generate_metrics_and_plots(df_final, k, l)\n",
        "\n",
        "    # 3. Gerar Readme\n",
        "    create_readme()\n",
        "\n",
        "    print(\"\\n##### Processo de Anonimização Concluído #####\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XudMD3qI1Es5",
        "outputId": "f1da0024-3440-4bf0-dedb-831994a2c23d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##### Início do Processo de Anonimização #####\n",
            "Processando com 1.0% dos dados...\n",
            "Reduzindo dataset de 855709 para 1.0%...\n",
            "Pré-processamento concluído. Registros finais para uso: 8557\n",
            "\n",
            "#################################################\n",
            "### Processando para K = 4\n",
            "#################################################\n",
            "Iniciando agrupamento k-NN para k=4...\n",
            "Agrupamento concluído. Total de clusters: 2139\n",
            "Aplicando generalização (k-anonimato)...\n",
            "Generalização concluída. Dataset anonimizado gerado com 8557 linhas.\n",
            "\n",
            "--- Aplicando L = 3 (para K = 4) ---\n",
            "\n",
            "Aplicando l-diversidade para l=3...\n",
            "Aplicação de l-diversidade concluída.\n",
            "\n",
            "Arquivo salvo com sucesso em: data\\dados covid 4 3.csv\n",
            "\n",
            "--- Métricas Finais ---\n",
            "Precisão (1 - Perda de Informação): 0.9776\n",
            "Total de Classes de Equivalência: 961\n",
            "Tamanho Médio das Classes: 8.90\n",
            "Gráfico de k-anonimato salvo em: data\\hist_k_anon_k4.png\n",
            "Gráfico de l-diversidade salvo em: data\\hist_l_div_k4_l3.png\n",
            "\n",
            "Readme.txt criado com sucesso.\n",
            "\n",
            "##### Processo de Anonimização Concluído #####\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}